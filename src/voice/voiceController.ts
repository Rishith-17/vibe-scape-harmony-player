import { VoiceState, MusicController, NavControllerAdapter } from './types';
import { parseIntent, HELP_TEXT } from './nlu/intentParser';
import { WebSpeechAsr } from './asr/WebSpeechAsr';
import { PorcupineWebEngine } from './wake/PorcupineWebEngine';
import { TtsEngine } from './tts/tts';
import { EarconPlayer } from './EarconPlayer';
import { runCommand } from './commandRunner';
import { setPlayerReady, whenReady } from './playerGate';

/**
 * SINGLETON Voice Controller - Single Shared Microphone/ASR Instance
 * 
 * All activation paths (Tap-Mic, Wake-word, Gesture) reuse the SAME mic/ASR instance.
 * No duplicate audio streams, overlays, or permission requests.
 * 
 * Principles:
 * - One mic/ASR instance only (singleton pattern)
 * - Wake and gesture ONLY signal this controller - they never create mic resources
 * - armMic() is the ONLY place where getUserMedia() may be called
 * - All activations produce identical behavior and UI state
 */

// Module-level singleton state - ensures single shared instance across all imports
let sharedMediaStream: MediaStream | null = null;
let sharedAsrEngine: WebSpeechAsr | null = null;
let sharedTtsEngine: TtsEngine | null = null;
let sharedEarconPlayer: EarconPlayer | null = null;
let sharedWakeEngine: PorcupineWebEngine | null = null;
let isAsrArmed = false;
let isListening = false;
let lastStartTimestamp = 0;
const DUPLICATE_SUPPRESSION_MS = 300;

// Stable instance ID for verification - generated once when first armed
let ASR_INSTANCE_ID: string | null = null;

/**
 * Main voice controller - orchestrates wake ‚Üí ASR ‚Üí NLU ‚Üí action flow
 * Now implements singleton pattern with shared resources
 */
export class VoiceController {
  private state: VoiceState = 'idle';
  private stateChangeCallback: ((state: VoiceState) => void) | null = null;
  private wakeEnabled = true;
  private musicController: MusicController;
  private navController: NavControllerAdapter;
  private config: {
    language: string;
    wakeSensitivity: number;
    ttsEnabled: boolean;
    wakeEnabled?: boolean;
  };

  constructor(
    musicController: MusicController,
    navController: NavControllerAdapter,
    config: {
      language: string;
      wakeSensitivity: number;
      ttsEnabled: boolean;
      wakeEnabled?: boolean;
    }
  ) {
    this.musicController = musicController;
    this.navController = navController;
    this.config = config;
    this.wakeEnabled = config.wakeEnabled !== false;

    // Initialize shared resources if not already created
    this.initializeSharedResources();
  }

  /**
   * Initialize shared resources (singleton pattern)
   * Called once by the first VoiceController instance
   */
  private initializeSharedResources(): void {
    if (!sharedTtsEngine) {
      sharedTtsEngine = new TtsEngine();
      console.debug('[VoiceController] üîä Created shared TTS engine');
    }

    if (!sharedEarconPlayer) {
      sharedEarconPlayer = new EarconPlayer();
      console.debug('[VoiceController] üîî Created shared Earcon player');
    }

    if (!sharedWakeEngine) {
      sharedWakeEngine = new PorcupineWebEngine(this);
      console.debug('[VoiceController] üéØ Created shared Wake engine');
    }

    // Setup engine callbacks
    this.setupEngines();
  }

  /**
   * Setup engine callbacks - called during initialization
   */
  private setupEngines(): void {
    // Wake word detection callback
    if (sharedWakeEngine) {
      sharedWakeEngine.onDetection(() => {
        console.debug('[VoiceController] üé§ Wake word "Hello Vibe" detected');
        this.onWakeDetected();
      });
      sharedWakeEngine.setSensitivity(this.config.wakeSensitivity);
    }

    // ASR callbacks will be set when ASR is armed
  }

  /**
   * Setup ASR callbacks after arming
   */
  private setupAsrCallbacks(): void {
    if (!sharedAsrEngine) return;

    sharedAsrEngine.onResult((transcript, isFinal) => {
      if (isFinal) {
        console.debug('[VoiceController] Final transcript:', transcript);
        this.processTranscript(transcript);
      }
    });

    sharedAsrEngine.onError((error) => {
      console.error('[VoiceController] ‚ùå ASR error:', error);
      this.setState('error');
      if (sharedEarconPlayer) {
        sharedEarconPlayer.play('error');
      }
      setTimeout(() => this.reset(), 2000);
    });
  }

  async initialize(): Promise<void> {
    if (sharedEarconPlayer) {
      await sharedEarconPlayer.initialize();
    }
    console.debug('[VoiceController] ‚úÖ Initialized');
    
    // Mark player as ready
    setPlayerReady(true);
    
    // Log current configuration
    console.debug('[VoiceController] Config:', {
      language: this.config.language,
      wakeSensitivity: this.config.wakeSensitivity,
      ttsEnabled: this.config.ttsEnabled,
      wakeEnabled: this.wakeEnabled,
      asrArmed: isAsrArmed,
      instanceId: ASR_INSTANCE_ID
    });
  }

  /**
   * PUBLIC API: Check if microphone has been armed by user
   * Returns true if user has granted permission and ASR instance is created
   */
  isMicArmed(): boolean {
    return isAsrArmed && sharedAsrEngine !== null;
  }

  /**
   * PUBLIC API: Get stable ASR instance ID for verification
   * Returns the same ID across all activation paths if using shared instance
   */
  getAsrInstanceId(): string | null {
    return ASR_INSTANCE_ID;
  }

  /**
   * PUBLIC API: Arm microphone - requests permission and creates shared ASR instance
   * This is the ONLY place where getUserMedia() may be called
   * Called by Tap-Mic on first user interaction
   */
  async armMic(): Promise<void> {
    // Already armed - reuse existing instance
    if (isAsrArmed && sharedAsrEngine) {
      console.debug('[VoiceController] ‚úÖ Mic already armed, reusing instance', ASR_INSTANCE_ID);
      return;
    }

    try {
      console.debug('[VoiceController] üé§ Arming mic - requesting permission...');
      
      // THIS IS THE ONLY getUserMedia() CALL IN THE ENTIRE VOICE SYSTEM
      sharedMediaStream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      
      console.debug('[VoiceController] ‚úÖ Microphone permission granted');

      // Create shared ASR instance (singleton)
      if (!sharedAsrEngine) {
        sharedAsrEngine = new WebSpeechAsr(this.config.language);
        
        // Generate stable instance ID for verification
        ASR_INSTANCE_ID = `ASR-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
        
        console.debug('[VoiceController] üé§ Created shared ASR instance:', ASR_INSTANCE_ID);
        
        // Setup callbacks
        this.setupAsrCallbacks();
      }

      isAsrArmed = true;
      console.debug('[VoiceController] ‚úÖ Mic armed successfully - ready for all activation paths');
      
    } catch (error) {
      console.error('[VoiceController] ‚ùå Failed to arm mic:', error);
      isAsrArmed = false;
      throw error;
    }
  }

  async start(): Promise<void> {
    try {
      // Start wake word detection only if enabled
      if (this.wakeEnabled && sharedWakeEngine) {
        await sharedWakeEngine.start();
        console.debug('[VoiceController] ‚úÖ Voice control ready');
        console.debug('[VoiceController] üé§ Say "Hello Vibe" or tap microphone button');
      } else {
        console.debug('[VoiceController] ‚úÖ Voice control ready (Tap Mic only)');
      }
      this.setState('idle');
    } catch (error) {
      console.error('[VoiceController] ‚ùå Failed to start wake word:', error);
      console.debug('[VoiceController] ‚úÖ Falling back to Tap Mic only');
      this.setState('idle'); // Still allow manual trigger
    }
  }

  async stop(): Promise<void> {
    if (sharedWakeEngine) {
      await sharedWakeEngine.stop();
    }
    if (sharedAsrEngine && isListening) {
      await sharedAsrEngine.stop();
      isListening = false;
    }
    if (sharedTtsEngine) {
      sharedTtsEngine.cancel();
    }
    this.setState('idle');
    console.debug('[VoiceController] Stopped');
  }

  /**
   * Wake word detection callback - reuses shared ASR instance
   */
  private async onWakeDetected(): Promise<void> {
    console.debug('[VoiceController] üé§ Wake word "Hello Vibe" detected');
    
    // Check if mic is armed before starting
    if (!this.isMicArmed()) {
      console.warn('[VoiceController] ‚ö†Ô∏è Wake word detected but mic not armed - ignoring');
      console.warn('[VoiceController] üí° User must tap mic button first to grant permission');
      return;
    }

    console.debug('[VoiceController] ‚úÖ Mic armed, starting shared ASR instance:', ASR_INSTANCE_ID);
    await this.startListeningFromArmedMic('wake');
  }

  /**
   * PUBLIC API: Start listening using the already-armed shared ASR instance
   * 
   * This is called by:
   * - Tap-Mic button (after armMic())
   * - Wake word detection (Porcupine)
   * - Gesture control (open_hand)
   * 
   * CRITICAL: This method NEVER calls getUserMedia() or creates new SpeechRecognition
   * It only starts the already-created shared ASR instance
   * 
   * @param source - Optional source identifier for debugging ('tap'|'wake'|'gesture')
   */
  async startListeningFromArmedMic(source: string = 'unknown'): Promise<void> {
    const now = Date.now();
    
    console.debug('[VoiceController] üé§ startListeningFromArmedMic() called from:', source);
    console.debug('[VoiceController] üìä State:', {
      currentState: this.state,
      isArmed: isAsrArmed,
      isListening,
      instanceId: ASR_INSTANCE_ID,
      timeSinceLastStart: now - lastStartTimestamp,
      source
    });

    // Guard: Mic must be armed first
    if (!isAsrArmed || !sharedAsrEngine) {
      console.error('[VoiceController] ‚ùå Cannot start listening - mic not armed!');
      console.error('[VoiceController] üí° Call armMic() first (via Tap-Mic button)');
      return;
    }

    // Guard: Already listening or processing
    if (this.state === 'listening' || this.state === 'processing') {
      console.debug('[VoiceController] ‚ö†Ô∏è Already listening or processing - ignoring duplicate call');
      return;
    }

    // Guard: Prevent rapid duplicate calls (debounce within 300ms)
    if (now - lastStartTimestamp < DUPLICATE_SUPPRESSION_MS) {
      console.debug('[VoiceController] ‚ö†Ô∏è Duplicate call suppressed (within 300ms window)');
      return;
    }

    // Update timestamp
    lastStartTimestamp = now;

    console.debug('[VoiceController] ‚úÖ All guards passed - starting shared ASR instance:', ASR_INSTANCE_ID);
    console.debug('[VoiceController] üîÑ Setting state to listening...');
    this.setState('listening');
    isListening = true;
    
    // Play audio feedback
    if (sharedEarconPlayer) {
      console.debug('[VoiceController] üîä Playing earcon...');
      sharedEarconPlayer.play('listen');
    }
    
    try {
      console.debug('[VoiceController] üé§ Starting shared ASR engine...', ASR_INSTANCE_ID);
      await sharedAsrEngine.start();
      console.debug('[VoiceController] ‚úÖ ASR started successfully - listening for command');
    } catch (error) {
      console.error('[VoiceController] ‚ùå Failed to start ASR:', error);
      this.setState('error');
      isListening = false;
      if (sharedEarconPlayer) {
        sharedEarconPlayer.play('error');
      }
      setTimeout(() => this.reset(), 2000);
    }
  }

  private async processTranscript(transcript: string): Promise<void> {
    this.setState('processing');
    if (sharedAsrEngine) {
      await sharedAsrEngine.stop();
      isListening = false;
    }

    try {
      const intent = parseIntent(transcript);
      console.log('[VoiceController] Parsed intent:', intent);

      if (intent.confidence < 0.5) {
        await this.speak("Sorry, I didn't understand that.");
        if (sharedEarconPlayer) {
          sharedEarconPlayer.play('error');
        }
        this.reset();
        return;
      }

      await this.executeIntent(intent);
      if (sharedEarconPlayer) {
        sharedEarconPlayer.play('success');
      }
    } catch (error) {
      console.error('[VoiceController] Error processing command:', error);
      await this.speak('Sorry, something went wrong.');
      if (sharedEarconPlayer) {
        sharedEarconPlayer.play('error');
      }
    }

    this.reset();
  }

  private async executeIntent(intent: any): Promise<void> {
    const { action, slots } = intent;

    console.log('[VoiceController] üéØ Executing action:', action, 'Slots:', slots);

    try {
      // Use command runner to prevent overlapping commands
      const result = await runCommand(async () => {
        switch (action) {
          case 'play':
            console.log('[VoiceController] ‚ñ∂Ô∏è Playing music');
            await whenReady(async () => {
              await this.musicController.play();
            });
            await this.speak('Playing');
            return;

          case 'pause':
            console.log('[VoiceController] ‚è∏Ô∏è Pausing music');
            await whenReady(async () => {
              await this.musicController.pause();
            });
            await this.speak('Paused');
            return;

          case 'resume':
            console.log('[VoiceController] ‚ñ∂Ô∏è Resuming music');
            await whenReady(async () => {
              await this.musicController.resume();
            });
            await this.speak('Resuming');
            return;

          case 'next':
            console.log('[VoiceController] ‚è≠Ô∏è Next track');
            await whenReady(async () => {
              await this.musicController.next();
            });
            await this.speak('Next track');
            return;

          case 'previous':
            console.log('[VoiceController] ‚èÆÔ∏è Previous track');
            await whenReady(async () => {
              await this.musicController.previous();
            });
            await this.speak('Previous track');
            return;

          case 'volume_up':
            console.log('[VoiceController] üîä Volume up');
            this.musicController.adjustVolume(10);
            await this.speak('Volume up');
            return;

          case 'volume_down':
            console.log('[VoiceController] üîâ Volume down');
            this.musicController.adjustVolume(-10);
            await this.speak('Volume down');
            return;

          case 'volume_set':
            if (slots.volume !== undefined) {
              console.log('[VoiceController] üîä Setting volume to', slots.volume);
              this.musicController.setVolume(slots.volume);
              await this.speak(`Volume set to ${slots.volume}`);
            }
            return;

          case 'play_query':
            if (slots.query) {
              console.log('[VoiceController] üéµ Playing query:', slots.query);
              await whenReady(async () => {
                await this.musicController.playQuery(slots.query);
              });
              await this.speak(`Playing ${slots.query}`);
            }
            return;

          case 'play_mood':
            if (slots.mood) {
              console.log('[VoiceController] üòä Playing mood:', slots.mood);
              await whenReady(async () => {
                await this.musicController.playMood(slots.mood);
              });
              await this.speak(`Playing ${slots.mood} music`);
            }
            return;

          case 'play_nth_track':
            if (slots.trackNumber) {
              console.log('[VoiceController] üéµ Playing track number:', slots.trackNumber);
              await whenReady(async () => {
                await this.musicController.playNthTrack(slots.trackNumber);
              });
              await this.speak(`Playing track ${slots.trackNumber}`);
            }
            return;

          case 'play_liked_songs':
            console.log('[VoiceController] ‚ù§Ô∏è Playing liked songs');
            await whenReady(async () => {
              await this.musicController.playLikedSongs();
            });
            await this.speak('Playing your liked songs');
            return;

          case 'play_playlist':
            if (slots.playlistName) {
              console.log('[VoiceController] üìã Playing playlist:', slots.playlistName);
              await whenReady(async () => {
                await this.musicController.playPlaylist(slots.playlistName);
              });
              await this.speak(`Playing ${slots.playlistName} playlist`);
            }
            return;

          case 'search_and_play':
            if (slots.query) {
              console.log('[VoiceController] üîç Searching and playing:', slots.query);
              await whenReady(async () => {
                await this.musicController.searchAndPlay(slots.query);
              });
              await this.speak(`Playing ${slots.query}`);
            }
            return;

          case 'search':
            if (slots.query) {
              console.log('[VoiceController] üîç Opening search for:', slots.query);
              this.navController.openSearch(slots.query);
              await this.speak(`Searching for ${slots.query}`);
            }
            return;

          case 'navigate':
            console.log('[VoiceController] üß≠ Navigating to:', slots.navigation);
            if (slots.navigation === 'emotions') this.navController.openEmotionDetection();
            else if (slots.navigation === 'library') this.navController.openLibrary();
            else if (slots.navigation === 'settings') this.navController.openSettings();
            else if (slots.navigation === 'home') this.navController.openHome();
            else if (slots.navigation === 'search') this.navController.openSearch();
            await this.speak(`Opening ${slots.navigation}`);
            return;

          case 'help':
            console.log('[VoiceController] ‚ÑπÔ∏è Showing help');
            await this.speak(HELP_TEXT);
            return;

          default:
            console.log('[VoiceController] ‚ùì Unknown action:', action);
            await this.speak("I don't know how to do that yet.");
            return;
        }
      });

      if (result === null) {
        console.log('[VoiceController] ‚è∏Ô∏è Command skipped - another command is running');
        await this.speak('Please wait, processing previous command');
      }
    } catch (error: any) {
      console.error('[VoiceController] ‚ùå Error executing intent:', error);
      await this.speak(error.message || 'Sorry, I could not complete that action.');
      throw error;
    }
  }

  private async speak(text: string): Promise<void> {
    if (!this.config.ttsEnabled || !sharedTtsEngine) return;

    this.setState('speaking');
    try {
      await sharedTtsEngine.speak(text, this.config.language);
    } catch (error) {
      console.error('[VoiceController] TTS error:', error);
    }
  }

  private reset(): void {
    this.setState('idle');
  }

  private setState(state: VoiceState): void {
    this.state = state;
    if (this.stateChangeCallback) {
      this.stateChangeCallback(state);
    }
  }

  onStateChange(callback: (state: VoiceState) => void): void {
    this.stateChangeCallback = callback;
  }

  getState(): VoiceState {
    return this.state;
  }

  updateConfig(config: Partial<typeof this.config>): void {
    Object.assign(this.config, config);
    if (config.wakeSensitivity !== undefined && sharedWakeEngine) {
      sharedWakeEngine.setSensitivity(config.wakeSensitivity);
    }
    if (config.language !== undefined && sharedAsrEngine) {
      sharedAsrEngine.setLanguage(config.language);
    }
    if (config.wakeEnabled !== undefined) {
      this.wakeEnabled = config.wakeEnabled;
      // Restart if necessary
      if (this.state === 'idle') {
        this.stop().then(() => this.start());
      }
    }
  }

  destroy(): void {
    this.stop();
    if (sharedWakeEngine) {
      sharedWakeEngine.stop();
    }
    if (sharedEarconPlayer) {
      sharedEarconPlayer.destroy();
    }
    setPlayerReady(false);
  }

  /**
   * PUBLIC API: Manual trigger for Tap-Mic button
   * Arms mic on first call, then starts listening on subsequent calls
   */
  async manualTrigger(): Promise<void> {
    console.debug('[VoiceController] üé§ Manual trigger (Tap-Mic) - ASR_ID:', ASR_INSTANCE_ID);
    
    // Arm mic if not already armed (first tap)
    if (!this.isMicArmed()) {
      console.debug('[VoiceController] üîì First tap - arming mic and requesting permission...');
      await this.armMic();
      console.debug('[VoiceController] ‚úÖ Mic armed - now starting listening...');
    }
    
    // Start listening using the shared ASR instance
    await this.startListeningFromArmedMic('tap');
  }

  /**
   * DEPRECATED: Use startListeningFromArmedMic() directly
   * Kept for backward compatibility
   */
  async stopListening(): Promise<void> {
    if (sharedAsrEngine && isListening) {
      await sharedAsrEngine.stop();
      isListening = false;
      this.setState('idle');
    }
  }
}
